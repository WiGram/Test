"""
Date:    February 17th, 2019
Authors: Kristian Strand and William Gram
Subject: Testing error terms
Ass. scripts: descriptive.py, genData.py

Description:
We continue off from the descriptive.py script.
We are testing if the return processes are generated
by simple normal processes:

y_t = u + eps_t

The error terms are then given by:

eps_t = y_t - u

These error terms are tested for normality.
"""

import genData as gd
import pandas as pd
from matplotlib import pyplot as plt
# import statsmodels.tsa.api as smt

plt.rcParams.update(plt.rcParamsDefault)
plt.style.use('seaborn-paper')
import numpy as np
np.set_printoptions(suppress = True)   # Disable scientific notation

# Generate the data
prices, monthlyRets, excessMRets, colNames, assets, monthlyVol, retCov, rf, pDates, rDates = gd.genData()

# Moments and quartiles of return processes
summaryRets = excessMRets.copy()
summaryRets['Risk Free'] = rf

# Describe mean and standard deviation (columns 1 and 2 in .describe)
summary = summaryRets.describe().transpose().iloc[:,1:3]

# ============================================= #
# ===== Analysis of error terms =============== #
# ============================================= #

mu = summary['mean'][:6]

epsilon = excessMRets - np.array(mu)

# Time series plot of each error term process
fig, axes = plt.subplots(nrows = 3, 
                         ncols = 2, 
                         sharex = True, 
                         sharey = True, 
                         figsize = (14,16))
fig.text(0.06, 0.5, 'Error', va='center', rotation='vertical')

test = np.array([epsilon.iloc[:,i] for i in range(assets)])
for ax, title, y in zip(axes.flat, colNames, test):
    ax.plot(rDates, y)
    ax.legend(loc = 'lower right')
    ax.set_title(title)
    ax.grid(False)
plt.savefig('/home/william/Dropbox/Thesis/Overleaf/images/Errors.pdf', bbox_inches = 'tight',
    pad_inches = 0)
plt.show()

# Notice large outlier for October 1987:
# S&P return was -25.4054
epsilon.iloc[:,5].min()

# Happened at index 50 = October 30, 1987:
idx = epsilon.iloc[:,5].values.argmin()
epsilon.index[idx]

"""
When doing misspecification testing we could consider removing this observation.

The argument will not be good, but would be that such a crash is unlikely, and
it disturbs our testing.
"""

import seaborn as sns
from scipy import stats
# Density plots for each return process
e_long_df = epsilon.stack().reset_index(name = 'Returns')
e_long_df = e_long_df.rename(columns={'level_1':'Asset class',})
g = sns.FacetGrid(data = e_long_df, col = 'Asset class', col_wrap = 2, height = 4, aspect = 1.5)
g.map(sns.distplot, 'Returns', fit = stats.norm)
plt.savefig('/home/william/Dropbox/Thesis/Overleaf/images/ErrorsDensities.pdf', bbox_inches = 'tight',
    pad_inches = 0)
plt.show()

# ============================================= #
# ===== Test for no auto-correlation ========== # 
# ============================================= #

"""
1 of three tests:
Test that gam_0 = 0 in the following:

eps_t = gam_0 eps_t-1 + u_t

If cannot be rejected: no auto-correlation passed.
"""
import statsmodels.api as sm
eps_t   = epsilon.iloc[1:,:].copy()
eps_lag = epsilon.iloc[:len(rDates)-1, :].copy()

# Runs OLS estimation on each of 'i' assets
test = [sm.OLS(eps_t[i].values, eps_lag[i].values).fit() for i in colNames]

# Save summary statistics, accessed by tables[i] for i in 0, 1, ..., assets - 1
tables = [test[i].summary() for i in range(assets)]

# Beta coefficients
[print('beta: ', test[i].params, x) for i, x in enumerate(colNames)]

# std errors
[print('Std. err.: ', test[i].bse, x) for i, x in enumerate(colNames)]

# t-values
[print('t-value: ', test[i].tvalues, x) for i, x in enumerate(colNames)]

# p-values
[print('p-value: ', test[i].pvalues, 'pass: ', test[i].pvalues > 0.05,  x) for i, x in enumerate(colNames)]

"""
The above tells us that Commodities, Russell 1000 and S&P 500 return 
processes could in fact have zero autocorrelation in the error term 
generated by a return process generated simply by a normal distribution.
"""

# LM test: T * R^2 ~ X^2(1)
T = eps_lag.shape[0] # Takes the amount of observations in each time series
[print('T * R^2: ', T * test[i].rsquared.round(5), 'passes: ', T * test[i].rsquared.round(3) < 3.841, x) for i, x in enumerate(colNames)]

# ============================================= #
# ===== Test for no-heteroskedasticity ======== #
# ============================================= #

"""
Cannot be done for our simple model:

eps_t^2 = gam_0 + gam_1 x_t + ... + gam_p x_t^2 + ... + u_t
"""

# ============================================= #
# ===== Test for normality of error term ====== #
# ============================================= #

"""
Consider the residuals u_t in:
eps_t = gam_0 eps_lag + u_t

We are now testing the third and fourth moments of: u_t

Tests of third and fourth moments:
xi_s = T / 6 * S^2         ~ Chi^2(1) = 3.841 (5 pct. level)
xi_k = T / 24 * (K - 3)^2  ~ Chi^2(1) = 3.841 (5 pct. level)

Combined: Jarque-Bera joint test of normality:
xi_JB = xi_s + xi_k ~ Chi^2(2) = 5.991 (5 pct. level)
"""

# To my knowledge: we must print the tables and hard copy the numbers
resid = [test[i].resid for i in range(assets)]
import scipy
S = [scipy.stats.skew(resid[i]) for i in range(assets)]
K = [scipy.stats.kurtosis(resid[i]) + 3 for i in range(assets)] # correct for excess kurtosis

xi_s = [T / 6 * S[i] ** 2 for i in range(assets)]
xi_k = [T / 24 * (K[i] - 3) ** 2 for i in range(assets)]
[print('Jarque Bera: ', round(xi_s[i] + xi_k[i], 5), 'pass: ', xi_s[i] + xi_k[i] < 5.991, x) for i, x in enumerate(colNames)]

"""
They all fail the JB test of normal residuals.

We could now consider removing October 30, 1987 from the dataset and redo
the test.

We could also consider modelling y_t as autoregressive processes.
"""