"""
Date:    February 17th, 2019
Authors: Kristian Strand and William Gram
Subject: Testing error terms
Ass. scripts: descriptive.py, genData.py

Description:
We continue off from the descriptive.py script.
We are testing if the return processes are generated
by simple normal processes:

y_t = u + eps_t

The error terms are then given by:

eps_t = y_t - u

These error terms are tested for normality.
"""

import genData as gd
import pandas as pd
from matplotlib import pyplot as plt
# import statsmodels.tsa.api as smt

plt.rcParams.update(plt.rcParamsDefault)
plt.style.use('seaborn-paper')
import numpy as np
np.set_printoptions(suppress = True)   # Disable scientific notation

# Generate the data
prices, monthlyRets, excessMRets, colNames, assets, monthlyVol, retCov, rf, pDates, rDates = gd.genData()

# Moments and quartiles of return processes
summaryRets = excessMRets.copy()
summaryRets['Risk Free'] = rf

# Describe mean and standard deviation (columns 1 and 2 in .describe)
summary = summaryRets.describe().transpose().iloc[:,1:3]

# ============================================= #
# ===== Analysis of error terms =============== #
# ============================================= #

mu = summary['mean'][:6]

epsilon = excessMRets - np.array(mu)

# Time series plot of each error term process
fig, axes = plt.subplots(nrows = 3, 
                         ncols = 2, 
                         sharex = True, 
                         sharey = True, 
                         figsize = (14,16))
fig.text(0.06, 0.5, 'Error', va='center', rotation='vertical')

test = np.array([epsilon.iloc[:,i] for i in range(assets)])
for ax, title, y in zip(axes.flat, colNames, test):
    ax.plot(rDates, y)
    ax.legend(loc = 'lower right')
    ax.set_title(title)
    ax.grid(False)
plt.savefig('/home/william/Dropbox/Thesis/Overleaf/images/Errors.pdf', bbox_inches = 'tight',
    pad_inches = 0)
plt.show()

# Notice large outlier for October 1987:
# S&P return was -25.4054
epsilon.iloc[:,5].min()

# Happened at index 50 = October 30, 1987:
idx = epsilon.iloc[:,5].values.argmin()
epsilon.index[idx]

"""
When doing misspecification testing we could consider removing this observation.

The argument will not be good, but would be that such a crash is unlikely, and
it disturbs our testing.
"""

import seaborn as sns
from scipy import stats
# Density plots for each return process
e_long_df = epsilon.stack().reset_index(name = 'Returns')
e_long_df = e_long_df.rename(columns={'level_1':'Asset class',})
g = sns.FacetGrid(data = e_long_df, col = 'Asset class', col_wrap = 2, height = 4, aspect = 1.5)
g.map(sns.distplot, 'Returns', fit = stats.norm)
plt.savefig('/home/william/Dropbox/Thesis/Overleaf/images/ErrorsDensities.pdf', bbox_inches = 'tight',
    pad_inches = 0)
plt.show()

# ============================================= #
# ===== Test for no auto-correlation ========== # 
# ============================================= #

"""
1 of three tests:
Test that gam_0 = 0 in the following:

eps_t = gam_0 eps_t-1 + u_t

If cannot be rejected: no auto-correlation passed.
"""
import statsmodels.api as sm
eps_t   = epsilon.iloc[1:,:].copy()
eps_lag = epsilon.iloc[:len(rDates)-1, :].copy()

# Runs OLS estimation on each of 'i' assets
test = [sm.OLS(eps_t[i].values, eps_lag[i].values).fit() for i in colNames]

# Save summary statistics, accessed by tables[i] for i in 0, 1, ..., assets - 1
tables = [test[i].summary() for i in range(assets)]

# Beta coefficients
[print('beta: ', test[i].params, x) for i, x in enumerate(colNames)]

# std errors
[print('Std. err.: ', test[i].bse, x) for i, x in enumerate(colNames)]

# t-values
[print('t-value: ', test[i].tvalues, x) for i, x in enumerate(colNames)]

# p-values
[print('p-value: ', test[i].pvalues, 'pass: ', test[i].pvalues > 0.05,  x) for i, x in enumerate(colNames)]

"""
The above tells us that Commodities, Russell 1000 and S&P 500 return 
processes could in fact have zero autocorrelation in the error term 
generated by a return process generated simply by a normal distribution.
"""

# LM test: T * R^2 ~ X^2(1)
T = eps_lag.shape[0] # Takes the amount of observations in each time series
[print('T * R^2: ', T * test[i].rsquared.round(5), 'passes: ', T * test[i].rsquared.round(3) < 3.841, x) for i, x in enumerate(colNames)]

# ============================================= #
# ===== Test for no-heteroskedasticity ======== #
# ============================================= #

"""
Cannot be done for our simple model:

eps_t^2 = gam_0 + gam_1 x_t + ... + gam_p x_t^2 + ... + u_t
"""

# ============================================= #
# ===== Test for normality of error term ====== #
# ============================================= #

"""
Consider the residuals u_t in:
eps_t = gam_0 eps_lag + u_t

We are now testing the third and fourth moments of: u_t

Tests of third and fourth moments:
xi_s = T / 6 * S^2         ~ Chi^2(1) = 3.841 (5 pct. level)
xi_k = T / 24 * (K - 3)^2  ~ Chi^2(1) = 3.841 (5 pct. level)

Combined: Jarque-Bera joint test of normality:
xi_JB = xi_s + xi_k ~ Chi^2(2) = 5.991 (5 pct. level)
"""

# To my knowledge: we must print the tables and hard copy the numbers
resid = [(test[i].resid - np.mean(test[i].resid) / np.std(test[i].resid) for i in range(assets)]
S = [stats.skew(resid[i]) for i in range(assets)]
K = [stats.kurtosis(resid[i]) + 3 for i in range(assets)] # correct for excess kurtosis

xi_s = [T / 6 * S[i] ** 2 for i in range(assets)]
xi_k = [T / 24 * (K[i] - 3) ** 2 for i in range(assets)]
[print('Jarque Bera: ', round(xi_s[i] + xi_k[i], 5), 'pass: ', xi_s[i] + xi_k[i] < 5.991, x) for i, x in enumerate(colNames)]

"""
They all fail the JB test of normal residuals.

We could now consider removing October 30, 1987 from the dataset and redo
the test.

We could also consider modelling y_t as autoregressive processes.
"""

# ============================================= #
# ===== Consider AR models of any lags ======== #
# ============================================= #

test_lags = 12

JB  = np.zeros((test_lags, assets))
gammaTable = JB.copy()
passGam = JB.copy()
passBG = JB.copy()
passNH = JB.copy()
RES = JB.copy()
passJB = JB.copy()

chi2 = np.array([
    3.841,
    5.991,
    7.815,
    9.488,
    11.071,
    12.592,
    14.067,
    15.507,
    16.919,
    18.307,
    19.675,
    21.026,
    22.362,
    23.685,
    24.996,
    26.296,
    27.587,
    28.869,
    30.144,
    31.410,
    32.671,
    33.924,
    35.172,
    36.415
])

for m, c in enumerate(colNames):
    for j in range(test_lags):
        
        lags = j + 1

        y   = excessMRets[c]
        x   = [y.shift(i)[lags:] for i in range(lags)]

        X = np.column_stack(x)
        Y = y[lags:]

        # First model Y = beta_1 x_1 + ... + beta_n x_n + eps_t
        model = sm.OLS(Y, X).fit()

        # Retrieve eps_t
        resids = model.resid

        # Add lagged residuals to all regressors
        resX = np.column_stack((X[1:,:], resids[1:]))

        # Run the model
        resModel = sm.OLS(resids[1:], resX).fit()

        # Retrieve gamma and p-stat
        gamma = resModel.params[j + 1]
        pVal  = resModel.pvalues[j + 1]
        gammaTable[j,m] = resModel.summary()
        passGam[j, m] = bool(pVal < 0.05)

        # Length of auxiliary regression eps_t = ...
        T = resX.shape[0]

        # Breusch-Godfrey LM test
        r2 = resModel.rsquared
        lm1 = T * r2

        # Pass if less than Chi^2
        passBG[j,m] = lm1 < chi2[0]

        # If the LM1 test is passed, we should test for higher degrees
        # of autocorrelation

        # Test for No-Heteroskedasticity
        resids2 = resids **2
        X2 = X ** 2

        # New auxiliary regression: eps^2 = mu + b x + b2 x^2 + u_t
        resX2 = np.column_stack((X, X2))
        test = sm.add_constant(resX2)
        res2Model = sm.OLS(resids2, resX2).fit()

        # We want all coefficients except for constant to be zero,
        # i.e. b11 = b12 = ... = b1k = b21 = b22 = ... = b2k = 0
        # This is tested against a Chi^2(2k) distribution, k being
        # amount of lags.
        r22 = res2Model.rsquared
        lm2 = T * r22

        # Pass if less than Chi^2
        passNH[j,m] = lm2 < chi2[2 * (lags - 1) + 1] # ends at index k-1

        # Define u = (eps_t - bar(eps)) / sigma(eps): all being ests
        u = (resids - np.mean(resids)) / np.std(resids)        
        
        s = stats.skew(u)
        k = stats.kurtosis(u)

        xi_s = T / 6 * s ** 2
        xi_k = T / 24 * k ** 2

        JB[j,m] = xi_s + xi_k

        passJB[j,m] = JB[j,m] < chi2[1]


pd.DataFrame(JB, index = range(1,13), columns = colNames)
pd.DataFrame(passGam, index = range(1,13), columns = colNames)
pd.DataFrame(passBG, index = range(1,13), columns = colNames)
pd.DataFrame(passNH, index = range(1,13), columns = colNames)
pd.DataFrame(passJB, index = range(1,13), columns = colNames)